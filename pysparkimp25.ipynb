{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6gjKkHVkuvutjGtfXi1cY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutoshghodke/ML_PROJECT/blob/main/pysparkimp25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70LiB9bFofu0",
        "outputId": "158da4bc-e6a0-489d-8e21-5fe943537ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=ffde2bea692f7a8ec699b81de4f1ccd3ecdcd364601b155137765abb405c19c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('practical').master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "y7tqhWWbo1Z2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfizp_oQpSY5",
        "outputId": "ca57a022-00f9-4b23-dd22-33f117dd63df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f17fef7bd00>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=spark.sparkContext.parallelize([1,2,3,4])"
      ],
      "metadata": {
        "id": "DS3d5qZUCR5L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0v8q3JGCoHa",
        "outputId": "2d52526c-5fc8-4716-f79e-1987ac172faf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=rdd.map(lambda x:x**2)"
      ],
      "metadata": {
        "id": "tTQcfTQaCpvW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jJyWoivDf-Q",
        "outputId": "f8b74980-2a78-4f51-cefa-2f5346a449c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 9, 16]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=[\"This is an sample application\"]\n",
        "v=spark.sparkContext.parallelize(rdd2)\n",
        "rdd3=v.flatMap(lambda x:x.split(' '))"
      ],
      "metadata": {
        "id": "9DqwHcZcDh1f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur3m44jvEOeE",
        "outputId": "b080f7ba-9e21-4165-f5f0-ffa3621d994e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'an', 'sample', 'application']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3,4,5,6]\n",
        "b = [7,8,9,4,5,6]"
      ],
      "metadata": {
        "id": "mk3FiuZ8EjLd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1=spark.sparkContext.parallelize(a)\n",
        "b1=spark.sparkContext.parallelize(b)"
      ],
      "metadata": {
        "id": "Jy1kIOvsE4pn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=a1.union(b1)\n",
        "c.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi24V0NPFCPB",
        "outputId": "debb2db8-7f3d-4456-97e2-4649d36a1a9d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a1.intersection(b1).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVpyIDRTFQRg",
        "outputId": "c7e2f61d-8c26-4248-f69b-65a41acaabe3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c.distinct().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNJJqPpxFVyX",
        "outputId": "ed1e5721-dff5-4804-a5d7-f2af49cef184"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 8, 1, 5, 9, 2, 6, 3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### groupByKey\n",
        "x = spark.sparkContext.parallelize([(\"a\",1),(\"s\",5),(\"a\",5),(\"s\",4)])\n"
      ],
      "metadata": {
        "id": "uL3MPvn-FdPO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1=x.groupByKey().mapValues(len)\n"
      ],
      "metadata": {
        "id": "zbs_4uP-jXYu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC-Qowm8kAi0",
        "outputId": "b8c15472-68e9-4828-d02a-8444ad1b1816"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('s', 2), ('a', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reduceByKey\n",
        "x2 = spark.sparkContext.parallelize([(\"a\",1),(\"s\",5),(\"a\",5),(\"s\",4)])\n",
        "x3=x2.reduceByKey(lambda x,y:x+y)\n",
        "x3.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAd_8fllkRoz",
        "outputId": "a61f4505-6c16-4b5e-da5d-9176142e920c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('s', 9), ('a', 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 11 sortByKey():\n",
        "x4 = spark.sparkContext.parallelize([(\"a\",1),(\"s\",5),(\"a\",5),(\"s\",4)])\n",
        "x5=x4.sortByKey()\n",
        "x5.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dyBQoSJkp0G",
        "outputId": "e5504335-3b0b-4da4-bc32-e3a03a6b021a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('a', 5), ('s', 5), ('s', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12 join():\n",
        "#       it combines fields from two table using copmmon values\n",
        "\n",
        "x = spark.sparkContext.parallelize([(\"a\",1),(\"s\",5),(\"a\",5),(\"s\",4)])\n",
        "y = spark.sparkContext.parallelize([(\"a\",1),(\"s\",5),(\"a\",5),(\"s\",4)])\n",
        "n=x.join(y)\n",
        "n.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skJ24tHulF6-",
        "outputId": "e4636302-664c-455c-8e85-c633fac30a5a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('s', (5, 5)),\n",
              " ('s', (5, 4)),\n",
              " ('s', (4, 5)),\n",
              " ('s', (4, 4)),\n",
              " ('a', (1, 1)),\n",
              " ('a', (1, 5)),\n",
              " ('a', (5, 1)),\n",
              " ('a', (5, 5))]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13 coalesce():\n",
        "#          to avoid full shuffling of data we use coalesce() function.\n",
        "\n",
        "x = spark.sparkContext.parallelize([\"jan\",'feb','mar','april','may','jun'],4)\n",
        "a=x.coalesce(3)          #====> 3 represent no. of partittion\n",
        "a.collect()\n",
        "a.getNumPartitions()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vvHhJXjlg7g",
        "outputId": "ff512956-482f-4e6a-908a-5335c2fd4876"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The coalesce will decrease the number of partitions\n",
        "# The repartition will increase the number of partitions\n"
      ],
      "metadata": {
        "id": "y3eVM0ublxdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"13 repartition()\"\"\"\n",
        "b=a.repartition(8)\n",
        "b.collect()\n",
        "b.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yJ1tx8MmNkD",
        "outputId": "20b6067a-f6b0-4e8b-da1f-8d640afd6acf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [\"001,Yash,Patil,29,M,011,20000\",\n",
        "\"002,Ram,Wagh,30,M,012,30000\",\n",
        "\"003,Sita,Patil,29,F,011,50000\",\n",
        "\"004,Kiran,More,33,M,011,40000\",\n",
        "\"005,Sachin,Tendulkar,51,M,012,100000\"]"
      ],
      "metadata": {
        "id": "xk-zgx_smgjw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.sparkContext.parallelize(a)\n"
      ],
      "metadata": {
        "id": "MqBDRpPvmrj7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# genderwise count\n",
        "rdd.map(lambda x:x.split(\",\")).map(lambda x: (x[4],1)).reduceByKey(lambda x,y:x+y).collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGdObsjJmt33",
        "outputId": "67222da7-b883-4619-bd52-402f903f9936"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('M', 4), ('F', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# genderwise salary\n",
        "rdd.map(lambda x:x.split(\",\")).map(lambda x: (x[4],int(x[6]))).reduceByKey(lambda x,y:x+y).collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfd_KsJwm1xl",
        "outputId": "e06026b5-430c-4467-b6ae-093f064581a4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('M', 190000), ('F', 50000)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second highest salary\n",
        "\n",
        "a =rdd.map(lambda x:x.split(\",\")).map(lambda x:int(x[6]))\n",
        "c = a.max()\n",
        "print(c)\n",
        "\n",
        "print(a.filter(lambda x:x != c).max())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7dPVkt2nGoT",
        "outputId": "4965e51e-7cde-469d-a914-904f7cae064d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n",
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # broadcast variables\n",
        "# broadcast varibles are read only shared variables that are cached and available on all nodes in cluster\n",
        "# in order to access by the task.\n",
        "\n",
        "# accumulator\n",
        "# Accumulators are write only shared variables and used to perform add and counter operations\n"
      ],
      "metadata": {
        "id": "-d8Q-farnNl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # broadcast variables\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "\n",
        "b_s = spark.sparkContext.broadcast(states)\n",
        "\n",
        "def states_con(code):\n",
        "    return b_s.value[code]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "\n",
        "rdd.map(lambda x: (x[0],x[1],x[2],states_con(x[3]))).collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_VT3PS2ne6c",
        "outputId": "2263dc98-d413-4719-d050-e39476df9ab1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('James', 'Smith', 'USA', 'California'),\n",
              " ('Michael', 'Rose', 'USA', 'New York'),\n",
              " ('Robert', 'Williams', 'USA', 'California'),\n",
              " ('Maria', 'Jones', 'USA', 'Florida')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accumulator\n",
        "accu = spark.sparkContext.accumulator(0)\n",
        "\n",
        "a = [1,2,3,4]\n",
        "rdd = spark.sparkContext.parallelize(a)\n",
        "\n",
        "rdd.foreach(lambda x: accu.add(x))\n",
        "\n",
        "print(accu.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2WnybMPnuY0",
        "outputId": "82370f75-198e-4c9d-c994-ec28186551db"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot()\n",
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
        "\n",
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8cFNjAaoHdF",
        "outputId": "d898aa18-8b51-48a9-9e57-7fb309fd07cb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-------+\n",
            "|Product|Amount|Country|\n",
            "+-------+------+-------+\n",
            "| Banana|  1000|    USA|\n",
            "|Carrots|  1500|    USA|\n",
            "|  Beans|  1600|    USA|\n",
            "| Orange|  2000|    USA|\n",
            "| Orange|  2000|    USA|\n",
            "| Banana|   400|  China|\n",
            "|Carrots|  1200|  China|\n",
            "|  Beans|  1500|  China|\n",
            "| Orange|  4000|  China|\n",
            "| Banana|  2000| Canada|\n",
            "|Carrots|  2000| Canada|\n",
            "|  Beans|  2000| Mexico|\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('product').pivot('country').sum('amount').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7g3NwB1WpdXS",
        "outputId": "cc6ed578-6df5-44ef-d430-5f975a0a21bb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+------+----+\n",
            "|product|Canada|China|Mexico| USA|\n",
            "+-------+------+-----+------+----+\n",
            "| Orange|  null| 4000|  null|4000|\n",
            "|  Beans|  null| 1500|  2000|1600|\n",
            "| Banana|  2000|  400|  null|1000|\n",
            "|Carrots|  2000| 1200|  null|1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 row_number() --used to give sequential number\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "a = Window.partitionBy('dept').orderBy(col('salary').desc())\n",
        "\n",
        "sh = df.withColumn('number',row_number().over(a))\n",
        "sh.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ej0SVxc7qACW",
        "outputId": "0da1f6cb-d8f7-477b-bf25-9c0c1edd7f8d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-eb0698e6ea8e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dept'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'salary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rank()  --used to give ranking .\n",
        "# --if same values are there it gives them same rank and\n",
        "#         for the next value rank will be summation of previous rank +\n",
        "#         no. of duplicates of previous rank\n",
        "\n",
        "r = df.withColumn('rank',rank().over(Window.orderBy(col('salary').desc()))).show()\n"
      ],
      "metadata": {
        "id": "wH-8KNHfpi7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dense_rank() ---used to give ranking\n",
        "\n",
        "#   if there are same values it gives same rank to them & for next value it gives next rank\n",
        "\n",
        "\n",
        "a = Window.partitionBy('dept').orderBy(col('salary').desc())\n",
        "\n",
        "df.withColumn('ranking',dense_rank().over(a)).where(col('ranking')==2).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9mktO7ugqI40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# joins\n",
        "\n",
        "# inner join - matching records\n",
        "# df.join(df1, df.name == df1.name,'inner').show()\n",
        "\n",
        "\n",
        "# left/ left_outer\n",
        "# df.join(df1,df.name == df1.name, 'left').show()\n",
        "\n",
        "# right / right_outer\n",
        "df.join(df1, df.name == df1.name, 'right').show()\n",
        "\n",
        "\n",
        "# leftsemi\n",
        "# df.join(df1,df.name == df1.name, 'leftsemi').show()    # ignores right dataset\n",
        "\n",
        "# leftanti\n",
        "# df.join(df1, df.name == df1.name , 'leftanti').show()   # returns non matching recoerds from left dataset\n",
        "\n",
        "# full / full_outer\n",
        "# df.join(df1, df.name == df1.name , 'full').show()\n"
      ],
      "metadata": {
        "id": "aPOLecRzqb7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cast\n",
        "\n",
        "df1 = df.select(df.bonus.cast('string'))\n",
        "\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "id": "Al7sK3YDqiZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# withcolumn\n",
        "\n",
        "# cast()    -change datatype of column\n",
        "df1 = df.withColumn('salary',col('salary').cast('Integer'))\n",
        "\n",
        "# add new column using existing column\n",
        "df.withColumn('salarys', col('salary')*10).show()\n",
        "\n",
        "# lit()  - add new column with default values\n",
        "df.withColumn('none',lit('null')).show()"
      ],
      "metadata": {
        "id": "GqNzaAcHqjqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#count()\n",
        "df.groupBy('dept').count().show()\n",
        "\n",
        "# sum()\n",
        "# df.groupBy('state').sum('bonus').show()\n",
        "\n",
        "# max()\n",
        "# df.groupBy('dept').max('salary').show()\n",
        "\n",
        "# min()\n",
        "# df.groupBy('dept').min('salary').show()\n",
        "\n",
        "# avg()\n",
        "# df.groupBy('dept').avg('salary').show()\n",
        "\n",
        "\n",
        "#mean()\n",
        "# df.groupBy('dept').mean('salary').show()\n",
        "\n",
        "\n",
        "# .agg\n",
        "\n",
        "# df.groupBy('dept').agg(sum('salary').alias('total'),\n",
        "#                        count('salary').alias('count'),\n",
        "#                       max('salary').alias('max'),\n",
        "#                       min('salary').alias('min'),\n",
        "#                       avg('salary').alias('avg')).show()"
      ],
      "metadata": {
        "id": "Tqm99pSFq1dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# salary greater than avg sal\n",
        "\"\"\"1\"\"\"\n",
        "df1 = df.select(avg('salary').cast('Integer')).collect()[0][0]\n",
        "print(df1)\n",
        "\n",
        "df.filter(df.salary > df1).show()"
      ],
      "metadata": {
        "id": "XAYvztJPq2bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuWUoHdrq6Ml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}